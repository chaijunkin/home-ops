---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app remediation
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
  install:
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    controllers:
      remediation:
        type: cronjob
        cronjob:
          schedule: "0 0 30 2 *"  # Feb 30th (never) - manual trigger only
          concurrencyPolicy: Forbid
          successfulJobsHistory: 1
          failedJobsHistory: 1
        containers:
          app:
            image:
              repository: ghcr.io/chaijunkin/linux-utility
              tag: rolling@sha256:8f71e42242dcf3563434fbaa2cd3bac3b2d02ed93ba2cbd7e12bfce822770b77
            command:
              - /bin/sh
              - -c
              - |
                # Install dependencies
                # Tools are pre-installed in the image; no need to install at runtime

                # Query Prometheus for VolSyncVolumeOutOfSync alerts
                alerts=$(curl -s "http://prometheus-operated.observability.svc.cluster.local:9090/api/v1/alerts")

                # Filter and iterate
                echo "$alerts" | jq -r '.data.alerts[] | select(.labels.alertname == "VolSyncVolumeOutOfSync") | "\(.labels.obj_namespace) \(.labels.obj_name)"' | while read -r namespace name; do
                  if [ -n "$namespace" ] && [ -n "$name" ]; then
                    echo "Processing $namespace/$name"

                    # Check if volsync job has pods in error state
                    job_name="volsync-src-${name}"
                    echo "Checking for failed pods in job $namespace/$job_name"

                    # Jobs can create multiple pods when retrying (e.g., 5 failed, 1 running)
                    # Only unlock if there are ONLY failed pods with no Running/Succeeded attempts
                    should_unlock="false"

                    # Check for any Running or Succeeded pods
                    active_pods=$(kubectl get pods -n "$namespace" -l job-name="$job_name" -o jsonpath='{range .items[*]}{.status.phase}{"\n"}{end}' 2>/dev/null | grep -E "Running|Succeeded" || true)

                    # Check for failed pods
                    failed_phase=$(kubectl get pods -n "$namespace" -l job-name="$job_name" -o jsonpath='{range .items[*]}{.status.phase}{"\n"}{end}' 2>/dev/null | grep -E "Failed|Error" || true)

                    # Check container states (waiting with reason like CrashLoopBackOff, Error, etc)
                    error_containers=$(kubectl get pods -n "$namespace" -l job-name="$job_name" -o jsonpath='{range .items[*].status.containerStatuses[*]}{.state.waiting.reason}{"\n"}{.state.terminated.reason}{"\n"}{end}' 2>/dev/null | grep -E "CrashLoopBackOff|Error|ImagePullBackOff" || true)

                    # Debug output
                    echo "Debug: active_pods='$active_pods' failed_phase='$failed_phase' error_containers='$error_containers'"

                    # Only unlock if we have errors AND no active pods
                    if ([ -n "$failed_phase" ] || [ -n "$error_containers" ]) && [ -z "$active_pods" ]; then
                      should_unlock="true"
                      echo "Found only failed pods in $namespace/$job_name with no active attempts - likely a restic lock issue"
                    elif [ -n "$active_pods" ]; then
                      echo "Found active pods in $namespace/$job_name - skipping unlock, letting current attempt finish"
                    fi

                    if [ "$should_unlock" = "true" ]; then

                      # Extract app name by removing -r2 suffix (ReplicationSource is ${APP}-r2, secret is ${APP}-volsync-r2)
                      app_name="${name%-r2}"

                      # Create unlock job for r2 repository
                      echo "Creating unlock job for $namespace/$app_name"
                      cat <<EOF | kubectl apply -f -
                apiVersion: batch/v1
                kind: Job
                metadata:
                  name: volsync-unlock-${app_name}-r2
                  namespace: ${namespace}
                spec:
                  ttlSecondsAfterFinished: 3600
                  template:
                    spec:
                      automountServiceAccountToken: false
                      restartPolicy: OnFailure
                      containers:
                        - name: unlock-r2
                          image: docker.io/restic/restic:latest
                          args: ["unlock", "--remove-all"]
                          envFrom:
                            - secretRef:
                                name: ${app_name}-volsync-r2
                          resources: {}
                EOF
                      echo "Waiting 5 seconds for unlock job to complete..."
                      sleep 5
                    else
                      echo "No unlock needed for $namespace/$job_name - no errors detected or job completed successfully"
                    fi

                    # Now trigger manual sync and cleanup
                    echo "Triggering manual sync for $namespace/$name"
                    kubectl patch replicationsource "$name" -n "$namespace" --type merge -p "{\"spec\":{\"trigger\":{\"manual\":\"$(date +%s)\"}}}"
                    # Remove finalizers from the VolumeSnapshot and delete it
                    kubectl patch volumesnapshot "volsync-$name-src" -n "$namespace" -p '{"metadata":{"finalizers":[]}}' --type merge || true
                    kubectl delete volumesnapshot "volsync-$name-src" -n "$namespace" --ignore-not-found
                  fi
                done
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: {drop: ["ALL"]}
            resources:
              requests:
                cpu: 10m
              limits:
                memory: 128Mi
        pod:
          restartPolicy: OnFailure
        serviceAccount:
          identifier: *app
    service:
      app:
        controller: *app
        enabled: false
    serviceAccount:
      remediation:
        enabled: true
    rbac:
      roles:
        list:
          type: ClusterRole
          forceRename: *app
          rules:
            - apiGroups: ["volsync.backube"]
              resources: ["replicationsources"]
              verbs: ["get", "list", "patch"]
            - apiGroups: ["snapshot.storage.k8s.io"]
              resources: ["volumesnapshots"]
              verbs: ["delete", "get", "list", "watch", "patch"]
            - apiGroups: ["batch"]
              resources: ["jobs"]
              verbs: ["create", "get", "list", "delete"]
            - apiGroups: [""]
              resources: ["pods"]
              verbs: ["get", "list"]
      bindings:
        list:
          type: ClusterRoleBinding
          forceRename: *app
          roleRef:
            kind: ClusterRole
            name: *app
          subjects:
            - kind: ServiceAccount
              name: *app
              namespace: "{{ .Release.Namespace }}"
