---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app remediation
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
  install:
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    controllers:
      remediation:
        type: cronjob
        cronjob:
          schedule: "0 0 30 2 *"  # Feb 30th (never) - manual trigger only
          concurrencyPolicy: Forbid
          successfulJobsHistory: 1
          failedJobsHistory: 1
        containers:
          app:
            image:
              repository: ghcr.io/chaijunkin/linux-utility
              tag: rolling@sha256:8f71e42242dcf3563434fbaa2cd3bac3b2d02ed93ba2cbd7e12bfce822770b77
            command:
              - /bin/sh
              - -c
              - |
                # Install dependencies
                # Tools are pre-installed in the image; no need to install at runtime

                # Query Prometheus for VolSyncVolumeOutOfSync alerts
                alerts=$(curl -s "http://prometheus-operated.observability.svc.cluster.local:9090/api/v1/alerts")

                # Filter and iterate
                echo "$alerts" | jq -r '.data.alerts[] | select(.labels.alertname == "VolSyncVolumeOutOfSync") | "\(.labels.obj_namespace) \(.labels.obj_name)"' | while read -r namespace name; do
                  if [ -n "$namespace" ] && [ -n "$name" ]; then
                    echo "Triggering manual sync for $namespace/$name"
                    kubectl patch replicationsource "$name" -n "$namespace" --type merge -p "{\"spec\":{\"trigger\":{\"manual\":\"$(date +%s)\"}}}"
                    # Remove finalizers from the VolumeSnapshot and delete it
                    kubectl patch volumesnapshot "volsync-$name-src" -n "$namespace" -p '{"metadata":{"finalizers":[]}}' --type merge || true
                    kubectl delete volumesnapshot "volsync-$name-src" -n "$namespace" --ignore-not-found
                  fi
                done
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: {drop: ["ALL"]}
            resources:
              requests:
                cpu: 10m
              limits:
                memory: 128Mi
        pod:
          restartPolicy: OnFailure
        serviceAccount:
          identifier: *app
    service:
      app:
        controller: *app
        enabled: false
    serviceAccount:
      remediation:
        enabled: true
    rbac:
      roles:
        list:
          type: ClusterRole
          forceRename: *app
          rules:
            - apiGroups: ["volsync.backube"]
              resources: ["replicationsources"]
              verbs: ["get", "list", "patch"]
            - apiGroups: ["snapshot.storage.k8s.io"]
              resources: ["volumesnapshots"]
              verbs: ["delete", "get", "list", "watch", "patch"]
      bindings:
        list:
          type: ClusterRoleBinding
          forceRename: *app
          roleRef:
            kind: ClusterRole
            name: *app
          subjects:
            - kind: ServiceAccount
              name: *app
              namespace: "{{ .Release.Namespace }}"
