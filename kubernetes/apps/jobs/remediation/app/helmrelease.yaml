---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app remediation
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
  install:
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    controllers:
      remediation:
        type: cronjob
        cronjob:
          schedule: "0 0 30 2 *"  # Feb 30th (never) - manual trigger only
          concurrencyPolicy: Forbid
          successfulJobsHistory: 1
          failedJobsHistory: 1
        containers:
          app:
            image:
              repository: ghcr.io/chaijunkin/linux-utility
              tag: rolling@sha256:8e1fde2ace5bbab379a53b97768dac83456a2bef02825ad9d8deca46f1d30856
            command:
              - /bin/sh
              - -c
              - |
                # Install dependencies
                # Tools are pre-installed in the image; no need to install at runtime

                # Query Prometheus for VolSyncVolumeOutOfSync alerts
                alerts=$(curl -s "http://prometheus-operated.observability.svc.cluster.local:9090/api/v1/alerts")

                # Filter and iterate
                echo "$alerts" | jq -r '.data.alerts[] | select(.labels.alertname == "VolSyncVolumeOutOfSync") | "\(.labels.obj_namespace) \(.labels.obj_name)"' | while read -r namespace name; do
                  if [ -n "$namespace" ] && [ -n "$name" ]; then
                    echo "Processing $namespace/$name"

                    # Check if volsync job has pods in error state
                    job_name="volsync-src-${name}"
                    echo "Checking for failed pods in job $namespace/$job_name"

                    # Get pods from the job and check for Error/Failed status
                    failed_pods=$(kubectl get pods -n "$namespace" -l job-name="$job_name" -o jsonpath='{range .items[*]}{.status.phase}{"\n"}{end}' 2>/dev/null | grep -E "Failed|Error" || true)

                    if [ -n "$failed_pods" ]; then
                      echo "Found failed pods in $namespace/$job_name - likely a restic lock issue"

                      # Extract app name by removing -r2 suffix (ReplicationSource is ${APP}-r2, secret is ${APP}-volsync-r2)
                      app_name="${name%-r2}"

                      # Create unlock job for r2 repository
                      echo "Creating unlock job for $namespace/$app_name"
                      cat <<EOF | kubectl apply -f -
                apiVersion: batch/v1
                kind: Job
                metadata:
                  name: volsync-unlock-${app_name}-r2
                  namespace: ${namespace}
                spec:
                  ttlSecondsAfterFinished: 3600
                  template:
                    spec:
                      automountServiceAccountToken: false
                      restartPolicy: OnFailure
                      containers:
                        - name: unlock-r2
                          image: docker.io/restic/restic:latest
                          args: ["unlock", "--remove-all"]
                          envFrom:
                            - secretRef:
                                name: ${app_name}-volsync-r2
                          resources: {}
                EOF
                      echo "Waiting 5 seconds for unlock job to complete..."
                      sleep 5
                    else
                      echo "No failed pods found in $namespace/$job_name - skipping unlock"
                    fi

                    # Now trigger manual sync and cleanup
                    echo "Triggering manual sync for $namespace/$name"
                    kubectl patch replicationsource "$name" -n "$namespace" --type merge -p "{\"spec\":{\"trigger\":{\"manual\":\"$(date +%s)\"}}}"
                    # Remove finalizers from the VolumeSnapshot and delete it
                    kubectl patch volumesnapshot "volsync-$name-src" -n "$namespace" -p '{"metadata":{"finalizers":[]}}' --type merge || true
                    kubectl delete volumesnapshot "volsync-$name-src" -n "$namespace" --ignore-not-found
                  fi
                done
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: {drop: ["ALL"]}
            resources:
              requests:
                cpu: 10m
              limits:
                memory: 128Mi
        pod:
          restartPolicy: OnFailure
        serviceAccount:
          identifier: *app
    service:
      app:
        controller: *app
        enabled: false
    serviceAccount:
      remediation:
        enabled: true
    rbac:
      roles:
        list:
          type: ClusterRole
          forceRename: *app
          rules:
            - apiGroups: ["volsync.backube"]
              resources: ["replicationsources"]
              verbs: ["get", "list", "patch"]
            - apiGroups: ["snapshot.storage.k8s.io"]
              resources: ["volumesnapshots"]
              verbs: ["delete", "get", "list", "watch", "patch"]
            - apiGroups: ["batch"]
              resources: ["jobs"]
              verbs: ["create", "get", "list", "delete"]
      bindings:
        list:
          type: ClusterRoleBinding
          forceRename: *app
          roleRef:
            kind: ClusterRole
            name: *app
          subjects:
            - kind: ServiceAccount
              name: *app
              namespace: "{{ .Release.Namespace }}"
