---
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaAlertRuleGroup
metadata:
  name: fly-app-alerts
spec:
  instanceSelector:
    matchLabels:
      grafana.internal/instance: grafana

  folderRef: "external-folder"
  interval: 1m
  name: "Fly App Monitoring"

  rules:
    # 1. High Response Time Alert
    - uid: fly-app-response-time
      title: "Fly App - High Response Time"
      condition: C
      data:
        - refId: A
          relativeTimeRange:
            from: 600
            to: 0
          datasourceUid: fly-io-prometheus
          model:
            expr: |
              histogram_quantile(0.95,
                sum by(app, status, le) (
                  rate(fly_app_http_response_time_seconds_bucket[5m])
                )
              )
            refId: A
        - refId: B
          datasourceUid: "-100"
          model:
            type: reduce
            refId: B
            expression: A
            reducer: last
        - refId: C
          datasourceUid: "-100"
          model:
            type: threshold
            refId: C
            expression: B
            conditions:
              - evaluator:
                  params: [5]
                  type: gt
      for: 5m
      noDataState: NoData
      execErrState: Alerting
      annotations:
        summary: "Response time exceeded 5 seconds for {{ $labels.app }}"
        description: "95th percentile response time is {{ $values.A.Value }}s. Consider declaring an incident."
        dashboard: "/d/eiRE4umnz/fly-app"
      labels:
        severity: critical
        alertname: HighResponseTime
        job: fly-app
        team: platform
        service: fly-app

    # 2. High Error Rate Alert
    - uid: fly-app-error-rate
      title: "Fly App - High Error Rate"
      condition: D
      data:
        - refId: A
          relativeTimeRange:
            from: 600
            to: 0
          datasourceUid: fly-io-prometheus
          model:
            expr: |
              sum by(app) (
                rate(fly_app_http_responses_count{status=~"5.*"}[5m])
              )
            refId: A
        - refId: B
          relativeTimeRange:
            from: 600
            to: 0
          datasourceUid: fly-io-prometheus
          model:
            expr: |
              sum by(app) (
                rate(fly_app_http_responses_count[5m])
              )
            refId: B
        - refId: C
          datasourceUid: "-100"
          model:
            type: math
            refId: C
            expression: "$A / $B * 100"
        - refId: D
          datasourceUid: "-100"
          model:
            type: threshold
            refId: D
            expression: C
            conditions:
              - evaluator:
                  params: [5]
                  type: gt
      for: 5m
      noDataState: NoData
      execErrState: Alerting
      annotations:
        summary: "Error rate above 5% for {{ $labels.app }}"
        description: "HTTP 5xx error rate is {{ $values.C.Value | humanizePercentage }}. Investigate immediately."
        dashboard: "/d/eiRE4umnz/fly-app"
      labels:
        severity: critical
        alertname: HighErrorRate
        job: fly-app
        team: platform
        service: fly-app

    # 3. Memory Pressure Alert
    - uid: fly-app-memory-pressure
      title: "Fly App - High Memory Pressure"
      condition: B
      data:
        - refId: A
          relativeTimeRange:
            from: 600
            to: 0
          datasourceUid: fly-io-prometheus
          model:
            expr: |
              fly_instance_memory_pressure_full{pressure_duration="avg60"}
            refId: A
        - refId: B
          datasourceUid: "-100"
          model:
            type: threshold
            refId: B
            expression: A
            conditions:
              - evaluator:
                  params: [50]
                  type: gt
      for: 10m
      noDataState: NoData
      execErrState: Alerting
      annotations:
        summary: "Memory pressure detected on {{ $labels.app }}"
        description: "Memory pressure avg60 is {{ $values.A.Value }}. Escalate to infrastructure team."
        dashboard: "/d/eiRE4umnz/fly-app"
      labels:
        severity: high
        alertname: HighMemoryPressure
        job: fly-app
        team: infrastructure
        service: fly-app

    # 4. Instance Down Alert
    - uid: fly-app-instance-down
      title: "Fly App - Instance Down"
      condition: B
      data:
        - refId: A
          relativeTimeRange:
            from: 300
            to: 0
          datasourceUid: fly-io-prometheus
          model:
            expr: |
              fly_instance_up
            refId: A
        - refId: B
          datasourceUid: "-100"
          model:
            type: threshold
            refId: B
            expression: A
            conditions:
              - evaluator:
                  params: [1]
                  type: lt
      for: 1m
      noDataState: Alerting
      execErrState: Alerting
      annotations:
        summary: "Instance {{ $labels.instance }} is down"
        description: "Fly.io instance for {{ $labels.app }} is unreachable."
        dashboard: "/d/eiRE4umnz/fly-app"
      labels:
        severity: critical
        alertname: InstanceDown
        job: fly-app
        team: platform
        service: fly-app

    # 5. CPU Throttling Alert
    - uid: fly-app-cpu-throttle
      title: "Fly App - CPU Throttling"
      condition: B
      data:
        - refId: A
          relativeTimeRange:
            from: 600
            to: 0
          datasourceUid: fly-io-prometheus
          model:
            expr: |
              rate(fly_instance_cpu_throttle[5m])
            refId: A
        - refId: B
          datasourceUid: "-100"
          model:
            type: threshold
            refId: B
            expression: A
            conditions:
              - evaluator:
                  params: [0]
                  type: gt
      for: 5m
      noDataState: NoData
      execErrState: Alerting
      annotations:
        summary: "CPU throttling on {{ $labels.app }}"
        description: "Instance {{ $labels.instance }} is experiencing CPU throttling. Consider scaling."
        dashboard: "/d/eiRE4umnz/fly-app"
      labels:
        severity: warning
        alertname: CPUThrottling
        job: fly-app
        team: platform
        service: fly-app

    # 6. Edge Error Rate Alert
    - uid: fly-app-edge-errors
      title: "Fly App - High Edge Error Rate"
      condition: B
      data:
        - refId: A
          relativeTimeRange:
            from: 600
            to: 0
          datasourceUid: fly-io-prometheus
          model:
            expr: |
              sum by(app, region) (
                rate(fly_edge_error_count[5m])
              )
            refId: A
        - refId: B
          datasourceUid: "-100"
          model:
            type: threshold
            refId: B
            expression: A
            conditions:
              - evaluator:
                  params: [10]
                  type: gt
      for: 5m
      noDataState: NoData
      execErrState: Alerting
      annotations:
        summary: "Edge errors in {{ $labels.region }} for {{ $labels.app }}"
        description: "Error rate is {{ $values.A.Value }}/sec in region {{ $labels.region }}."
        dashboard: "/d/eiRE4umnz/fly-app"
      labels:
        severity: medium
        alertname: HighEdgeErrorRate
        job: fly-app
        team: platform
        service: fly-app
